---
layout: default
title: Ke Lin
---


<h2>About Me</h2>

<div class="aboutme">
  <div class="aboutme-content">
    <p>
      I am a second-year master student in Software Engineering, Tsinghua University, supervised by <a
        href="https://www.thss.tsinghua.edu.cn/en/faculty/pingluo.htm">Prof. Ping Luo</a>. I received my
      bachelor's
      degree in Software Engineering from Tsinghua University, advised by <a
        href="https://www.thss.tsinghua.edu.cn/en/faculty/lijiewen.htm">Prof. Lijie Wen</a>.
    </p>
    <p>I am currently a research intern at <a href="https://gcloud.tencent.com/pages/research/game-ai.html">Tencent
        Game AI Research Center</a>, with a mission to improve
      the realistic, reliability, and aesthetic of AI-generated 3D scenes.</p>
    <p style="color: rgb(205,97,85)">ðŸŒŸSeeking open positions for 2025 Fall Ph.D.ðŸŒŸ</p>
  </div>
  <div><img class="profile-picture" src="img/profile.jpg" /></div>
</div>

<hr />

<h2>Education</h2>

<div class="education">
  <div class="education-block">
    <img src="img/ustc_logo.png" />
    <div>
      <div class="education-school">University of Science and Technology of China</div>
      <div class="education-date">2022.09 - 2025.06 (expected)</div>
      <div class="education-degree">Master of Engineering in Computer Science</div>
      <div class="education-score">GPA: 3.8/4.3</div>
    </div>
  </div>
  <div class="education-block">
    <img src="img/ustc_logo.png" />
    <div>
      <div class="education-school">University of Science and Technology of China</div>
      <div class="education-date">2018.09 - 2022.06</div>
      <div class="education-degree">Bachelor in Computer Science</div>
      <div class="education-score">GPA: 3.49/4.3, Rank 40%</div>
    </div>
  </div>
</div>

<hr />

<h2>Research of Interest</h2>

<p>Previously, I focused on AI safety, including LLM Watermark, Linguistic Steganography and Privacy-preserving
  Computation.</p>

<p>My current Research of Interest are:</p>
<ul>
  <li>3D Scene Layout</li>
  <li>LLM-guided 3D Vision</li>
  <li>Multi-modal 3D LLMs</li>
  <li>Compositional 3D Vision</li>
</ul>

<hr />

<!-- <h2>Research Experiences</h2> -->
<!-- 
<div class="research">
  <div class="research-block">
    <img src="img/tencent-logo.png" />
    <div>
      <div class="research-institute"><a href="https://gcloud.tencent.com/pages/research/game-ai.html">Tencent -
          Game AI Research Center</a></div>
      <div class="research-date">2024.04 - Present</div>
      <div class="research-role">Research Intern</div>
    </div>
  </div>
  <div class="research-block">
    <img src="img/momenta-logo.png" />
    <div>
      <div class="research-institute"><a href="https://www.momenta.cn/en/">Momenta</a></div>
      <div class="research-date">2021.01 - 2021.04</div>
      <div class="research-role">Research & Development Intern</div>
    </div>
  </div>
</div> -->

<!-- <hr /> -->

<h2>Selected Publications</h2>

<div class="publication">
  <p>* denotes equal contribution.</p>
  <div class="pub-block">
    <img src="img/paper/calpcg.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="https://arxiv.org/abs/2311.16501">
              Context-Aware Indoor Point Cloud Object Generation through User Instructions
            </a></div>
          <div class="pub-author">Yiyang Luo*, Ke Lin*, </b>Chao Gu</b></div>
          <div class="pub-conf">ACM MM 2024</div>
          <div>
            <span class="pub-tag">CCF A</span>
            <span class="pub-tag">CORE A*</span>
            <span class="pub-tag">3D Vision</span>
          </div>
        </div>
        <div class="pub-misc">
          <a href="https://github.com/AInnovateLab/Context-aware-Indoor-PCG" target="_blank">Code</a>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(calpcg_abs);">Abstract</a>
          |
          <a href="https://ainnovatelab.github.io/Context-aware-Indoor-PCG/" target="_blank">Project Page</a>
        </div>
      </div>
      <pre
        id="calpcg_abs">Indoor scene modification has emerged as a prominent area within computer vision, particularly for its applications in Augmented Reality (AR) and Virtual Reality (VR). Traditional methods often rely on pre-existing object databases and predetermined object positions, limiting their flexibility and adaptability to new scenarios. In response to this challenge, we present a novel end-to-end multi-modal deep neural network capable of generating point cloud objects seamlessly integrated with their surroundings, driven by textual instructions. Our work proposes a novel approach in scene modification by enabling the creation of new environments with previously unseen object layouts, eliminating the need for pre-stored CAD models. Leveraging Point-E as our generative model, we introduce innovative techniques such as quantized position prediction and Top-K estimation to address the issue of false negatives resulting from ambiguous language descriptions. Furthermore, we conduct comprehensive evaluations to showcase the diversity of generated objects, the efficacy of textual instructions, and the quantitative metrics, affirming the realism and versatility of our model in generating indoor objects. To provide a holistic assessment, we incorporate visual grounding as an additional metric, ensuring the quality and coherence of the scenes produced by our model. Through these advancements, our approach not only advances the state-of-the-art in indoor scene modification but also lays the foundation for future innovations in immersive computing and digital environment creation.</pre>
    </div>
  </div>
  <div class="pub-block">
    <img src="img/paper/wm_collision.jpg" />
    <div class="pub-content-wrapper">
      <div class="pub-content">
        <div class="pub-innerblock">
          <div class="pub-title"><a href="https://arxiv.org/abs/2403.10020">
              Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs
            </a></div>
          <div class="pub-author">Yiyang Luo*, Ke Lin*, <b>Chao Gu*</b>, Ping Luo, Lijie Wen, Jiahui
            Hou</div>
          <div class="pub-conf">Under Review</div>
          <div>
            <span class="pub-tag">Watermark</span>
          </div>
        </div>
        <div class="pub-misc">
          <span>Code (TBD)</span>
          |
          <a href="javascript:void(0);" onclick="javascript:toggle(wm_collision_abs);">Abstract</a>
        </div>
      </div>
      <pre
        id="wm_collision_abs">The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.</pre>
    </div>
  </div>
</div>

<hr />

<!-- <h2>Services</h2>

<ul>
  <li><b>Reviewer</b>: ACL, EMNLP, NAACL, MM, IJCAI</li>
  <li><b>Teaching Assistant</b>:
    <ul>
      <li>2021 Web Front-end Technology. Worked with <a
          href="https://www.thss.tsinghua.edu.cn/en/faculty/lijiewen.htm">Prof. Lijie Wen</a>.</li>
    </ul>
  </li>
</ul> -->
<!-- TA -->

<hr />

<h2>Awards and Scholarships</h2>

<ul>
  <li>2019 The Second Prize Scholarship</li>
</ul>